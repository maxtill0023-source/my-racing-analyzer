"""
kra_scraper.py â€” KRA ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ
ê³µê³µë°ì´í„°í¬í„¸ APIë¥¼ í†µí•´ ì¶œì „í‘œ, ì¡°êµ, ê²½ì£¼ë§ˆ ì •ë³´, ê²½ì£¼ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
API ë¶ˆê°€ ì‹œ KRA ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ í´ë°±ì„ ì œê³µí•©ë‹ˆë‹¤.
"""
import json
import os
import time
from datetime import datetime, timedelta
from urllib.parse import quote_plus

import pandas as pd
import requests
import requests
from bs4 import BeautifulSoup
import warnings
from io import StringIO

# Suppress FutureWarning for read_html
warnings.simplefilter(action='ignore', category=FutureWarning)

import config


class KRAScraper:
    """KRA ë°ì´í„° ìˆ˜ì§‘ê¸°"""

    def __init__(self):
        self.api_key = config.KRA_API_KEY
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Referer": "https://race.kra.co.kr/",
            "Origin": "https://race.kra.co.kr",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        })
        
        # ì„¸ì…˜ ì´ˆê¸°í™” (ì¿ í‚¤ íšë“)
        try:
            self.session.get("https://race.kra.co.kr/", timeout=5)
        except:
            pass

    # 5. [NEW] ì¶œì „í‘œìƒì„¸ì •ë³´ Web Scraping (API ëŒ€ì²´)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def scrape_race_entry_page(self, race_date: str, meet: str, race_no: str) -> pd.DataFrame:
        """
        ì¶œì „ìƒì„¸ì •ë³´ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ (chulmaDetailInfoChulmapyo.do)
        íŠ¹ì´ì‚¬í•­, ê¸°ì–´ ë³€ë™ ë“± APIì— ì—†ëŠ” ì •ë³´ í™•ë³´ ê°€ëŠ¥
        """
        url = "https://race.kra.co.kr/chulmainfo/chulmaDetailInfoChulmapyo.do"
        params = {
            "meet": meet,
            "rcDate": race_date,
            "rcNo": race_no
        }
        
        print(f"  [Scraping] Entry Page: {race_date} {meet}Race {race_no}")
        
        try:
            # Browser-like headers
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124 Safari/537.36",
                "Referer": "https://race.kra.co.kr/chulmainfo/chulmaDetailInfoChulmapyo.do"
            }
            
            # [FIX] Use requests.get directly to avoid Session encoding quirks
            import requests
            resp = requests.get(url, params=params, headers=headers, timeout=10)
            
            # [DEBUG] Inspect Response
            print(f"  [Debug] Final URL: {resp.url}")
            print(f"  [Debug] Headers: {dict(resp.headers)}")
            print(f"  [Debug] Content Length: {len(resp.content)}")
            print(f"  [Debug] Content Hex Prefix: {resp.content[:50].hex()}")
            
            # [FIX] Manually decode content
            try:
                html_text = resp.content.decode('cp949', errors='replace')
            except Exception:
                 html_text = resp.text
            
            # [FIX] Remove/Replace meta charset
            html_text = html_text.replace('euc-kr', 'utf-8').replace('EUC-KR', 'utf-8')
            
            # [FIX] Use BeautifulSoup for robust parsing
            from bs4 import BeautifulSoup
            
            soup = BeautifulSoup(html_text, 'html.parser')
            tables = soup.find_all('table')
            
            target_df = None
            
            # Strategy 1: Header Name Matching
            for table in tables:
                headers = []
                thead = table.find('thead')
                if thead:
                    header_row = thead.find('tr')
                    if header_row:
                        headers = [th.get_text(strip=True) for th in header_row.find_all(['th', 'td'])]
                
                if not headers:
                    first_tr = table.find('tr')
                    if first_tr:
                        headers = [td.get_text(strip=True) for td in first_tr.find_all(['td', 'th'])]
                
                # Check keywords (Mojibake might prevent this)
                if any("ë§ˆëª…" in h for h in headers) and any("ê¸°ìˆ˜" in h for h in headers):
                    # Found via headers
                    rows = []
                    tbody = table.find('tbody')
                    tr_list = tbody.find_all('tr') if tbody else table.find_all('tr')
                    
                    if not tbody and tr_list and tr_list[0] == table.find('tr'):
                         tr_list = tr_list[1:]
                         
                    for tr in tr_list:
                        cells = [td.get_text(strip=True) for td in tr.find_all('td')]
                        if len(cells) == len(headers):
                            rows.append(cells)
                        elif len(cells) > 0:
                            rows.append(cells + [''] * (len(headers) - len(cells)))
                            
                    target_df = pd.DataFrame(rows, columns=headers)
                    break 

            # Strategy 2: Index/Structure Matching (Fallback for Mojibake)
            if target_df is None:
                for table in tables:
                    rows = []
                    tbody = table.find('tbody')
                    tr_list = tbody.find_all('tr') if tbody else table.find_all('tr')
                    
                    # Heuristic: Entry table has many rows and ~15 columns
                    # Check first row col count
                    if tr_list:
                        first_cells = tr_list[0].find_all('td')
                        if len(first_cells) >= 12: # At least 12 columns
                             # Extract data without headers
                             for tr in tr_list:
                                 cells = [td.get_text(strip=True) for td in tr.find_all('td')]
                                 if len(cells) >= 12:
                                     rows.append(cells)
                             
                             if rows:
                                 # Construct DataFrame with dummy columns first
                                 max_len = max(len(r) for r in rows)
                                 cols = [f"Col{i}" for i in range(max_len)]
                                 # Pad rows
                                 rows_padded = [r + ['']*(max_len-len(r)) for r in rows]
                                 target_df = pd.DataFrame(rows_padded, columns=cols)
                                 
                                 # Map by Index (Standard KRA Layout)
                                 # 0:No, 1:Name, 6:Burden, 8:Weight, 11:Jockey, 12:Trainer
                                 rename_map = {
                                     cols[0]: "hrNo",
                                     cols[1]: "hrName",
                                     cols[6]: "wgBudam",
                                     cols[8]: "weight",
                                     cols[11]: "jkName",
                                     cols[12]: "trName"
                                 }
                                 target_df = target_df.rename(columns=rename_map)
                                 break

            if target_df is None:
                return pd.DataFrame()

            # Clean and Standardize Columns
            # If Strategy 1 worked, we need to rename map
            rename_map_std = {
                "ë²ˆí˜¸": "hrNo", "ë§ˆë²ˆ": "hrNo", "ìˆœìœ„": "hrNo",
                "ë§ˆëª…": "hrName",
                "ì„±ë³„": "sex",
                "ì—°ë ¹": "age",
                "ì¤‘ëŸ‰": "wgBudam", "ë¶€ë‹´ì¤‘ëŸ‰": "wgBudam",
                "ì²´ì¤‘": "weight", "ë§ˆì²´ì¤‘": "weight",
                "ê¸°ìˆ˜ëª…": "jkName", "ê¸°ìˆ˜": "jkName",
                "ì¡°êµì‚¬ëª…": "trName", "ì¡°êµì‚¬": "trName",
                "ë ˆì´íŒ…": "rating"
            }
            target_df = target_df.rename(columns=rename_map_std)

            # [FIX] Final Fallback: If hrNo/hrName missing (due to partial Mojibake), map by index
            if "hrNo" not in target_df.columns or "hrName" not in target_df.columns:
                 # Standard KRA Layout (0:No, 1:Name...)
                 # Check if we have enough columns
                 cols = target_df.columns
                 if len(cols) >= 12:
                     fallback_map = {
                         cols[0]: "hrNo",
                         cols[1]: "hrName",
                         cols[6]: "wgBudam", 
                         cols[8]: "weight",
                         cols[11]: "jkName",
                         cols[12]: "trName"
                     }
                     target_df = target_df.rename(columns=fallback_map)

            # ìˆ«ìí˜• ë³€í™˜ (ë²ˆí˜¸)
            if "hrNo" in target_df.columns:
                 target_df["hrNo"] = pd.to_numeric(target_df["hrNo"], errors='coerce').fillna(0).astype(int).astype(str)
                 
            # [FIX] Return Removed here to allow further processing
            
            # ìµœê·¼ ì „ì /íŠ¹ì´ì‚¬í•­ ì»¬ëŸ¼ ì°¾ê¸° (ìœ„ì¹˜ ê¸°ë°˜ ë˜ëŠ” í‚¤ì›Œë“œ)
            # ë³´í†µ 12ë²ˆì§¸ ì¸ë±ìŠ¤ ê·¼ì²˜ (ë²ˆí˜¸, ë§ˆëª…, ì‚°ì§€, ì„±ë³„, ì—°ë ¹, ë ˆì´íŒ…, ì¤‘ëŸ‰, ì¦ê°, ê¸°ìˆ˜, ì¡°êµì‚¬, ë§ˆì£¼, [ì¡°êµ], [ìµœê·¼ì „ì ], [ì¥êµ¬], [íŠ¹ì´])
            
            recent_col = next((c for c in target_df.columns if "ìµœê·¼" in c or "ì „ì " in c), None)
            note_col = next((c for c in target_df.columns if "ë¹„ê³ " in c or "ê¸°ì–´" in c or "íŠ¹ì´" in c), None)
            
            rename_map_extra = {}
            if recent_col:
                rename_map_extra[recent_col] = "recent_rank"
            if note_col:
                rename_map_extra[note_col] = "remark"
                
            if rename_map_extra:
                target_df = target_df.rename(columns=rename_map_extra)
            
            # [FIX] Ensure trName/jkName exist
            # Try to map by index if missing (Standard KRA layout: 11=Jockey, 12=Trainer)
            if "jkName" not in target_df.columns and len(target_df.columns) > 11:
                try: target_df = target_df.rename(columns={target_df.columns[11]: "jkName"})
                except: pass
            
            if "trName" not in target_df.columns and len(target_df.columns) > 12:
                try: target_df = target_df.rename(columns={target_df.columns[12]: "trName"})
                except: pass

            # [FIX] ë§¤í•‘ë˜ì§€ ì•Šì€ ì»¬ëŸ¼ ì¤‘ 12ë²ˆì§¸(ì¸ë±ìŠ¤ 12)ë¥¼ recent_rankë¡œ ê°•ì œ í• ë‹¹ (ì¸ì½”ë”© ë¬¸ì œ ëŒ€ë¹„)
            # ë‹¨, ì»¬ëŸ¼ ìˆ˜ê°€ ì¶©ë¶„í•  ë•Œë§Œ
            # WARN: Index 12 is usually Trainer. Recent rank is usually later (e.g. index 13 or 14).
            # ONLY map if we absolutely need to find it and it's not mapped yet.
            if "recent_rank" not in target_df.columns and len(target_df.columns) > 13:
                 # Try index 13 first
                 try:
                    target_df = target_df.rename(columns={target_df.columns[13]: "recent_rank"})
                 except: pass

            # Ensure all required columns exist (prevent KeyError in app.py)
            required_cols = ["hrNo", "hrName", "jkName", "trName", "remark", "rating"]
            for col in required_cols:
                if col not in target_df.columns:
                    target_df[col] = ""  # Default empty string

            # [FIX] Deduplicate columns (keep first occurrence)
            target_df = target_df.loc[:, ~target_df.columns.duplicated()]

            # [FIX] hrNoê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ hrNoë¡œ ê°„ì£¼ (ì¸ì½”ë”© ê¹¨ì§ ëŒ€ë¹„)
            if "hrNo" not in target_df.columns and not target_df.empty:
                print("  [Warn] 'hrNo' column missing. Using 1st column as 'hrNo'.")
                target_df = target_df.rename(columns={target_df.columns[0]: "hrNo"})

            # ìˆ«ìí˜• ë³€í™˜ (ë²ˆí˜¸) - Re-apply in case renames happened
            if "hrNo" in target_df.columns:
                 target_df["hrNo"] = pd.to_numeric(target_df["hrNo"], errors='coerce').fillna(0).astype(int).astype(str)
                 
            return target_df

        except Exception as e:
            print(f"  [Error] Scraping Entry Page: {e}")
            return pd.DataFrame()

    def scrape_steward_reports(self, race_date: str, meet: str, race_no: str) -> dict:
        """
        'ì‹¬íŒë¦¬í¬íŠ¸' íƒ­ ìŠ¤í¬ë˜í•‘ (chulmaDetailInfoStewardsReport.do)
        í˜„ì¬ ê²½ì£¼ì˜ ì „ ì¶œì „ë§ˆì— ëŒ€í•œ ê³¼ê±° ì‹¬íŒë¦¬í¬íŠ¸ë¥¼ 1ë²ˆ ìš”ì²­ìœ¼ë¡œ ìˆ˜ì§‘.
        
        ì£¼í–‰ ë°©í•´, ì§„ë¡œ ë¬¸ì œ, ê¼¬ë¦¬ê°ê¸° ë“±ì˜ ê¸°ë¡ì„ í†µí•´
        ì‹¤ë ¥ ì´ìƒìœ¼ë¡œ ìˆœìœ„ê°€ ë‚®ì•˜ë˜ ë§ˆí•„ì„ ì‹ë³„í•  ìˆ˜ ìˆìŒ.
        
        Returns:
            dict: {hrNo: [{"date": "2025/01/11-5R", "report": "ì‹¬íŒ ë³´ê³  ë‚´ìš©..."}, ...]}
        """
        try:
            url = "https://race.kra.co.kr/chulmainfo/chulmaDetailInfoStewardsReport.do"
            params = {"meet": meet, "rcDate": race_date, "rcNo": race_no}
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }

            print(f"  [Scraping] Steward Reports: {race_date} Meet{meet} Race{race_no}")
            resp = self.session.get(url, params=params, headers=headers, timeout=15)
            # [Fix] Use bytes + BS4 auto-detect or explicit from_encoding
            # Steward reports seem to be mixed or explicitly UTF-8
            
            soup = BeautifulSoup(resp.content, "html.parser", from_encoding="utf-8")
            tables = soup.find_all("table")
            
            result = {}  # {hrNo: [{"date": ..., "report": ...}]}
            
            # ì‹¬íŒë¦¬í¬íŠ¸ í…Œì´ë¸” ì°¾ê¸° (ë³´í†µ ë§ˆì§€ë§‰ í…Œì´ë¸”, 4ì»¬ëŸ¼: ë§ˆë²ˆ, ë§ˆëª…, ë‚ ì§œ, ë¦¬í¬íŠ¸)
            for tbl in tables:
                rows = tbl.find_all("tr")
                if len(rows) < 2:
                    continue
                
                # í—¤ë” í™•ì¸
                header_cells = [th.get_text(strip=True) for th in rows[0].find_all(["th", "td"])]
                if len(header_cells) != 4:
                    continue
                
                # ë°ì´í„° í–‰ íŒŒì‹±
                for row in rows[1:]:
                    cells = [td.get_text(strip=True) for td in row.find_all("td")]
                    if len(cells) < 4:
                        continue
                    
                    hr_no = cells[0].strip()
                    hr_name = cells[1].strip()
                    report_date = cells[2].strip()
                    report_text = cells[3].strip()
                    
                    if not hr_no or not hr_no.isdigit():
                        continue
                    
                    if hr_no not in result:
                        result[hr_no] = []
                    
                    # ë¦¬í¬íŠ¸ê°€ ì—†ëŠ” ë§ë„ ìˆìŒ (ë¹ˆ ì¤„)
                    if report_text:
                        result[hr_no].append({
                            "date": report_date,
                            "report": report_text,
                            "hrName": hr_name
                        })
            
            total_reports = sum(len(v) for v in result.values())
            horses_with_reports = sum(1 for v in result.values() if v)
            print(f"  [OK] Steward: {horses_with_reports}horses with {total_reports} reports")
            return result
            
        except Exception as e:
            print(f"  [Error] Steward Reports scraping: {e}")
            return {}

    def scrape_race_10score(self, race_date: str, meet: str, race_no: str) -> dict:
        """
        'ìµœê·¼ 10íšŒ ì „ì ' íƒ­ ìŠ¤í¬ë˜í•‘ (chulmaDetailInfo10Score.do)
        í•œ ë²ˆì˜ ìš”ì²­ìœ¼ë¡œ ì „ ì¶œì „ë§ˆì˜ ìµœê·¼ 10ì „ ê¸°ë¡ (S1F, G1F, ê¸°ë¡ ë“±) ìˆ˜ì§‘
        
        Returns:
            dict: {hrNo: [list of race records]} 
                  ê° recordëŠ” dict with keys: rcDate, ord, rcDist, rcTime, s1f, g3f, g1f, wgBudam, weight
        """
        try:
            url = "https://race.kra.co.kr/chulmainfo/chulmaDetailInfo10Score.do"
            params = {"meet": meet, "rcDate": race_date, "rcNo": race_no}
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }

            print(f"  [Scraping] 10 Recent Races: {race_date} Meet{meet} Race{race_no}")
            resp = self.session.get(url, params=params, headers=headers, timeout=15)
            if resp.encoding == 'ISO-8859-1':
                resp.encoding = resp.apparent_encoding
            resp.raise_for_status()

            soup = BeautifulSoup(resp.text, "html.parser")
            tables = soup.find_all("table")
            
            result = {}  # {hrNo: [records]}
            
            for tbl in tables:
                text = tbl.get_text()
                # ë°ì´í„° í…Œì´ë¸” ì‹ë³„: S-1F ì»¬ëŸ¼ì´ ìˆëŠ” í…Œì´ë¸”
                if "S-1F" not in text:
                    continue
                
                rows = tbl.find_all("tr")
                if len(rows) < 3:
                    continue
                
                # Row 0: ë§ ì •ë³´ í—¤ë” (ì˜ˆ: "[ì•”]  1íí”¼ë“œì‹œí¬  5 ì„¸  í•œêµ­  [ê¸°] ì¡°í•œë³„  53.5")
                header_text = rows[0].get_text(strip=True)
                
                # ë§ˆë²ˆ ì¶”ì¶œ (ì²« ë²ˆì§¸ ìˆ«ì)
                import re
                hr_match = re.search(r'(\d+)', header_text)
                if not hr_match:
                    continue
                hr_no = hr_match.group(1)
                
                # Row 1: ì»¬ëŸ¼ í—¤ë” (ìˆœ, ì¼ì, ê²½, ì£¼, ë“±, ê±°ë¦¬, ë‘ìˆ˜, ì°©, ìˆœìœ„/ë‘ìˆ˜, ê¸°ìˆ˜, ì¤‘ëŸ‰, S-1F, G-3F, G-1F, ê¸°ë¡, ì²´ì¤‘, ë ˆì´íŒ…, ì£¼)
                # Row 2+: ë°ì´í„° í–‰
                records = []
                for row in rows[2:]:
                    cells = [td.get_text(strip=True) for td in row.find_all("td")]
                    if len(cells) < 15:
                        continue
                    
                    try:
                        # ì‹œê°„ ë³€í™˜ í—¬í¼: "0:13.9" -> 13.9, "1:23.0" -> 83.0
                        def parse_time(t_str):
                            t_str = str(t_str).strip()
                            if ":" in t_str:
                                parts = t_str.split(":")
                                try:
                                    return float(parts[0]) * 60 + float(parts[1])
                                except: return 0
                            try: return float(t_str)
                            except: return 0

                        # S1F/G1FëŠ” ë³´í†µ "0:13.9" (200m) ì´ë¯€ë¡œ ì´ˆ ë¶€ë¶„ë§Œ í•„ìš”
                        s1f_raw = cells[11]
                        g1f_raw = cells[13]
                        s1f_sec = parse_time(s1f_raw)  # 0:13.9 -> 13.9
                        g1f_sec = parse_time(g1f_raw)  # 0:13.7 -> 13.7
                        
                        # ordë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
                        try:
                            ord_val = int(cells[2])
                        except:
                            ord_val = 99

                        record = {
                            "rcDate": cells[1].replace("/", "").split("-")[0] if "/" in cells[1] else cells[1],
                            "rcNo": cells[1].split("-")[1].replace("R", "") if "-" in cells[1] else "",
                            "ord": ord_val,  # ìˆœìœ„ (int)
                            "rcDist": cells[5],  # ê±°ë¦¬
                            "rcTime": cells[14],  # ê¸°ë¡ (ì›ë³¸ ìœ ì§€)
                            "s1f": s1f_sec,  # S-1F (ì´ˆ ë‹¨ìœ„ float)
                            "g3f": parse_time(cells[12]),  # G-3F (ì´ˆ ë‹¨ìœ„)
                            "g1f": g1f_sec,  # G-1F (ì´ˆ ë‹¨ìœ„ float)
                            "wgBudam": cells[10],  # ë¶€ë‹´ì¤‘ëŸ‰
                            "weight": cells[15] if len(cells) > 15 else "",  # ë§ˆì²´ì¤‘
                        }
                        records.append(record)
                    except (IndexError, ValueError):
                        continue
                
                if records:
                    result[hr_no] = records
                    
            print(f"  [OK] 10Score: {len(result)} horses scraped")
            return result
            
        except Exception as e:
            print(f"  [Error] 10Score scraping: {e}")
            return {}

    # -------------------------------------------------------------------------
    # Helper Methods
    # -------------------------------------------------------------------------
    def _call_api(self, url: str, params: dict, tag: str = "") -> list:
        """
        ê³µê³µë°ì´í„°í¬í„¸ API í˜¸ì¶œ ê³µí†µ í•¨ìˆ˜.
        Returns: list of dict (items)
        """
        params["serviceKey"] = self.api_key
        params.setdefault("_type", "json")
        params.setdefault("numOfRows", "100") # [REVERT] ê¸°ë³¸ê°’ 100ìœ¼ë¡œ ë³µêµ¬
        params.setdefault("pageNo", "1")

        try:
            resp = self.session.get(url, params=params, timeout=15)
            resp.raise_for_status()
        except requests.RequestException as e:
            print(f"  [API Error] {tag}: {e}")
            return []

        try:
            data = resp.json()
        except json.JSONDecodeError:
            # XML ì‘ë‹µì´ê±°ë‚˜ HTML ì—ëŸ¬ í˜ì´ì§€ì¸ ê²½ìš°
            print(f"  [JSON Error] {tag} - {resp.text[:200]}")
            return []

        # ê³µê³µë°ì´í„°í¬í„¸ í‘œì¤€ ì‘ë‹µ êµ¬ì¡° íŒŒì‹±
        body = data.get("response", {}).get("body", {})
        items = body.get("items", {})

        if not items:
            print(f"  [No Data] {tag}")
            return []

        item_list = items.get("item", [])
        # ë‹¨ì¼ ê±´ì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ê¸°
        if isinstance(item_list, dict):
            item_list = [item_list]

        return item_list

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1. ì¶œì „í‘œ ìƒì„¸ì •ë³´ (API + Full Scraping Fallback)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def fetch_race_entries(self, race_date: str, meet: str = "1") -> pd.DataFrame:
        """
        ì¶œì „í‘œ ìƒì„¸ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        API í‚¤ê°€ ì—†ê±°ë‚˜ í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ì›¹ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.
        """
        print(f"ğŸ“‹ ì¶œì „í‘œ ìˆ˜ì§‘ ì¤‘... (ë‚ ì§œ: {race_date}, ê²½ë§ˆì¥: {meet})")

        # API ì‚¬ìš© ì‹œë„ (í‚¤ê°€ ìˆì„ ë•Œë§Œ)
        if self.api_key and len(self.api_key) > 10:
            items = self._call_api(
                config.ENTRY_API,
                {"rc_date": race_date, "meet": meet, "numOfRows": 400}, # [FIX] í•˜ë£¨ ì „ì²´ ê²½ì£¼
                tag="ì¶œì „í‘œ"
            )
            if items:
                df = pd.DataFrame(items)
                print(f"  [Success] ì¶œì „í‘œ {len(df)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ (API)")
                return df

        print("  [Info] API ì‚¬ìš© ë¶ˆê°€ ë˜ëŠ” ë°ì´í„° ì—†ìŒ. ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œë„...")
        return self._scrape_entries_full(race_date, meet)

    def _scrape_entries_full(self, race_date: str, meet: str) -> pd.DataFrame:
        """KRA ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì¶œì „í‘œ ìŠ¤í¬ë˜í•‘ (í’€ ë²„ì „ - ê³¼ê±° ë°ì´í„°ëŠ” ê²½ì£¼ì„±ì í‘œ í™œìš©)"""
        print("  [Info] ê³¼ê±° ì¶œì „í‘œ ìŠ¤í¬ë˜í•‘ -> ê²½ì£¼ì„±ì í‘œ ìŠ¤í¬ë˜í•‘ ê²°ê³¼ í™œìš©")
        # ê²½ì£¼ ì„±ì í‘œ ìŠ¤í¬ë˜í•‘ ë¡œì§ ì¬ì‚¬ìš© (ì¶œì „ë§ˆ ë° ê¸°ë³¸ ì •ë³´ í™•ë³´)
        df = self._scrape_results_full(race_date, meet)
        
        if not df.empty:
            # [Fix] Data Leakage ë°©ì§€: ìˆœìœ„(ord) ë° ê²°ê³¼ ê´€ë ¨ ì»¬ëŸ¼ ì œê±° + ìˆœì„œ ì„ê¸°
            leak_cols = ["ord", "ë„ì°©ì°¨", "winOdds", "plcOdds", "time", "rcTime"]
            df = df.drop(columns=[c for c in leak_cols if c in df.columns], errors="ignore")
            
            # ìˆœì„œ ì„ê¸° (ìˆœìœ„ìˆœ ì •ë ¬ ë°©ì§€)
            df = df.sample(frac=1).reset_index(drop=True)
            print("  [Clean] ê²°ê³¼ ì»¬ëŸ¼ ì œê±° ë° ìˆœì„œ ì…”í”Œ ì™„ë£Œ (Data Leakage ë°©ì§€)")
            
        return df

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2. ì¼ì¼í›ˆë ¨ ìƒì„¸ì •ë³´ (ì¡°êµ í˜„í™©)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2. ì¼ì¼í›ˆë ¨ ìƒì„¸ì •ë³´ (ì¡°êµ í˜„í™©) - API + Web Scraping
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def fetch_training_data(self, train_date: str = None, meet: str = "1",
                            horse_name: str = None) -> pd.DataFrame:
        """
        ì¡°êµ(í›ˆë ¨) ë°ì´í„° ìˆ˜ì§‘ (API -> Web Fallback)
        """
        # API ì‚¬ìš© ì‹œë„
        if self.api_key and len(self.api_key) > 10:
            params = {"meet": meet}
            if train_date: params["tr_date"] = train_date
            if horse_name: params["hr_name"] = horse_name
            
            items = self._call_api(config.TRAINING_API, params, tag="ì¡°êµ")
            if items:
                df = pd.DataFrame(items)
                print(f"  [Success] ì¡°êµ ë°ì´í„° {len(df)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ (API)")
                return df

        # Web Fallback
        t_date = train_date if train_date else datetime.now().strftime("%Y%m%d")
        return self._scrape_training_daily(t_date, meet)

    def _scrape_training_daily(self, date: str, meet: str) -> pd.DataFrame:
        """KRA ì›¹ì‚¬ì´íŠ¸ ì¼ì¼ì¡°êµí˜„í™© ìŠ¤í¬ë˜í•‘"""
        try:
            # URL: seoul/trainer/dailyExerList.do (ì¡°êµì‚¬ë³„)
            base_url = "https://race.kra.co.kr/seoul/trainer/dailyExerList.do"
            if meet == "2": base_url = "https://race.kra.co.kr/busan/trainer/dailyExerList.do"
            elif meet == "3": base_url = "https://race.kra.co.kr/jeju/trainer/dailyExerList.do"
            
            params = {"meet": meet, "realDate": date}
            resp = self.session.get(base_url, params=params, timeout=5)
            # í…Œì´ë¸” íŒŒì‹±
            dfs = pd.read_html(StringIO(resp.text))
            
            all_rows = []
            for df in dfs:
                # "ë§ˆëª…"ê³¼ "ì¡°êµì‚¬" í˜¹ì€ "ê¸°ìˆ˜"ê°€ ìˆëŠ” í…Œì´ë¸”
                if "ë§ˆëª…" in str(df.columns) and ("ì¡°êµì‚¬" in str(df.columns) or "ê¸°ìˆ˜" in str(df.columns)):
                    rename_map = {
                        "ë§ˆëª…": "hrName", "ë§ˆ ë²ˆ": "hrNo",
                        "ì¡°êµì‚¬": "trName", "ê¸°ìˆ˜": "jkName",
                        "ì¡°êµì": "trName", "ì´íšŒìˆ˜": "runCount", "ì£¼ë¡œ": "track",
                        "êµ¬ë¶„": "trType",
                    }
                    df = df.rename(columns=rename_map)
                    df["trDate"] = date
                    all_rows.append(df)
            
            if all_rows:
                merged = pd.concat(all_rows, ignore_index=True)
                # ë°ì´í„° íƒ€ì… ì •ë¦¬
                merged["runCount"] = pd.to_numeric(merged["runCount"], errors="coerce").fillna(0)
                print(f"  [Success] ì›¹ ìŠ¤í¬ë˜í•‘ ì¡°êµ ë°ì´í„° {len(merged)}ê±´ ìˆ˜ì§‘")
                return merged

            return pd.DataFrame()
            
        except Exception as e:
            # print(f"  [Error] ì¡°êµ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}") # ë„ˆë¬´ ì‹œë„ëŸ¬ìš°ë©´ ì£¼ì„ ì²˜ë¦¬
            return pd.DataFrame()

    def fetch_training_for_week(self, race_date: str, meet: str = "1") -> pd.DataFrame:
        """ê²½ì£¼ì¼ ê¸°ì¤€ ìµœê·¼ 1ì£¼ê°„ ì¡°êµ ë°ì´í„° ìˆ˜ì§‘"""
        print(f"ğŸ‹ ê¸ˆì£¼ ì¡°êµ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        race_dt = datetime.strptime(race_date, "%Y%m%d")

        all_data = []
        for i in range(7):
            dt = race_dt - timedelta(days=i)
            date_str = dt.strftime("%Y%m%d")
            
            # ìœ„ì—ì„œ ì •ì˜í•œ fetch_training_data í˜¸ì¶œ (API or Web)
            df = self.fetch_training_data(train_date=date_str, meet=meet)
            if not df.empty:
                all_data.append(df)
            
            # API ì‚¬ìš©ì‹œëŠ” sleep, ì›¹ì€ sleep ëœ í•„ìš”í•˜ì§€ë§Œ ë§¤ë„ˆìƒ 0.2ì´ˆ
            time.sleep(0.2)

        if all_data:
            final_df = pd.concat(all_data, ignore_index=True)
            print(f"  [Success] ê¸ˆì£¼ ì¡°êµ ë°ì´í„° ì´ {len(final_df)}ê±´ ìˆ˜ì§‘")
            return final_df
        return pd.DataFrame()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 3. ê²½ì£¼ë§ˆ ìƒì„¸ì •ë³´ (ê³¼ê±° ì„±ì  í¬í•¨)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def fetch_horse_details(self, horse_name: str = None,
                            horse_no: str = None,
                            meet: str = "1") -> dict:
        """
        ê²½ì£¼ë§ˆ ìƒì„¸ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

        Args:
            horse_name: ë§ˆëª…
            horse_no: ë§ˆë²ˆ
            meet: ê²½ë§ˆì¥ ì½”ë“œ

        Returns:
            dict â€” ë§ˆí•„ ìƒì„¸ ì •ë³´ (ê³¼ê±° ì„±ì  í¬í•¨)
        """
        # [FIX] horseInfo APIëŠ” meet íŒŒë¼ë¯¸í„°ê°€ ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ì¶©ëŒì„ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìŒ
        # params = {"meet": meet} 
        params = {} 
        if horse_name:
            params["hr_name"] = horse_name
        if horse_no:
            params["hrNo"] = horse_no

        items = self._call_api(config.HORSE_API, params, tag=f"Horse-{horse_name or horse_no}")

        if items:
            return items[0] if len(items) == 1 else items
            
        # [FALLBACK] API ì‹¤íŒ¨ ì‹œ ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œë„
        if horse_no:
            print(f"  [Info] ê²½ì£¼ë§ˆ ìƒì„¸ API ì‹¤íŒ¨. ìŠ¤í¬ë˜í•‘ ì‹œë„... ({horse_no})")
            return self._scrape_horse_details(horse_no, meet)
            
        return {}

    def _scrape_horse_details(self, horse_no: str, meet: str) -> list[dict]:
        """
        ê²½ì£¼ë§ˆ ìƒì„¸ì •ë³´(ê³¼ê±° ì „ì ) ìŠ¤í¬ë˜í•‘
        URL: https://race.kra.co.kr/racehorse/profileRaceResult.do
        """
        try:
            url = "https://race.kra.co.kr/racehorse/profileRaceResult.do"
            params = {
                "meet": meet,
                "hrNo": horse_no
            }
            resp = self.session.get(url, params=params, timeout=5)
            resp.raise_for_status()
            
            dfs = pd.read_html(StringIO(resp.text))
            if not dfs:
                return []
                
            # 'ìˆœìœ„'ì™€ 'ê²½ì£¼ëª…' ë“±ì´ ìˆëŠ” í…Œì´ë¸” ì°¾ê¸°
            target_df = None
            for df in dfs:
                if "ìˆœìœ„" in str(df.columns) and "ê²½ì£¼ëª…" in str(df.columns):
                    target_df = df
                    break
            
            if target_df is not None:
                # ì»¬ëŸ¼ ë§¤í•‘ (API ì‘ë‹µ í‚¤ì™€ ë™ì¼í•˜ê²Œ ë§ì¶¤)
                # API Key: rcDate, rcNo, ord, rcTime, s1f, g1f, etc.
                # Web Cols: ê²½ì£¼\nì¼ì, ê²½ì£¼\në²ˆí˜¸, ìˆœìœ„, ...
                p_map = {
                    "ê²½ì£¼\nì¼ì": "rcDate", "ê²½ì£¼ì¼ì": "rcDate",
                    "ê²½ì£¼\në²ˆí˜¸": "rcNo", "ê²½ì£¼ë²ˆí˜¸": "rcNo",
                    "ìˆœìœ„": "ord", 
                    "ì£¼ë¡œ\nìƒíƒœ": "track", "ì£¼ë¡œ": "track",
                    "ê±°ë¦¬": "rcDist",
                    "ê¸°ë¡": "rcTime", "ê²½ì£¼\nê¸°ë¡": "rcTime",
                    "S1F": "s1f", "1ì½”ë„ˆ": "s1f", # ê·¼ì‚¬ì¹˜
                    "G1F": "g1f", "3ì½”ë„ˆ": "g1f_proxy", # G1Fê°€ ì—†ì„ ìˆ˜ ìˆìŒ
                    "ì°©ì°¨": "diff",
                    "ë¶€ë‹´\nì¤‘ëŸ‰": "wgBudam", "ì¤‘ëŸ‰": "wgBudam",
                    "ë§ˆì²´ì¤‘": "weight", "ì²´ì¤‘": "weight",
                    "ê¸°ìˆ˜": "jkName",
                    "ì¡°êµì‚¬": "trName"
                }
                
                # ì»¬ëŸ¼ëª… ë‹¨ìˆœí™” (ì¤„ë°”ê¿ˆ ì œê±°)
                target_df.columns = [str(c).replace(" ", "") for c in target_df.columns]
                
                target_df = target_df.rename(columns=p_map)
                
                # ì „ì²˜ë¦¬
                if "ord" in target_df.columns:
                    target_df["ord"] = pd.to_numeric(target_df["ord"], errors='coerce').fillna(99)
                
                # S1F, G1Fê°€ ì›¹ì— ì—†ì„ ê²½ìš° (ë³´í†µ ìƒì„¸ íŒì—…ì— ìˆìŒ)
                # ì¼ë‹¨ ìˆëŠ” ì •ë³´ë¼ë„ ë¦¬í„´í•´ì•¼ 'ì „ì  ì—†ìŒ'ì„ ë©´í•¨
                
                records = target_df.to_dict('records')
                # API í¬ë§· í˜¸í™˜ì„± ë³´ì •
                for r in records:
                    if "rcDate" in r:
                        r["rcDate"] = str(r["rcDate"]).replace("/", "").replace("-", "")
                        
                print(f"  [Success] ì›¹ ìŠ¤í¬ë˜í•‘ ê²½ì£¼ ê¸°ë¡ {len(records)}ê±´ ìˆ˜ì§‘")
                
                # [Added] Steward Reports (Bad Luck/Interference)
                try:
                    steward_reports = self.scrape_steward_reports(race_date, meet, rc_no)
                    # Merge report into records if possible?
                    # The records are list of dicts. We have {hrNo: [reports]}.
                    # Let's attach 'steward_report' field to the horse record if matches hrNo
                    for rec in records:
                        h_no = str(rec.get("hrNo", "")).strip()
                        if h_no in steward_reports:
                            # Attach the most recent report or list?
                            # For simplicity, attach the list
                            rec["steward_reports"] = steward_reports[h_no]
                            
                except Exception as e:
                    print(f"  [Warn] Steward report failed: {e}")

                return records

        except Exception as e:
            print(f"  [Scrape Error] Horse Info: {e}")
            
        return []

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # [NEW] 3-1. ì§„ë£Œ ë‚´ì—­ (Lung/Joint)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def fetch_medical_history(self, hr_no: str, hr_name: str) -> list[str]:
        """
        ìµœê·¼ 1ë…„ì¹˜ ì§„ë£Œ ë‚´ì—­ ì¡°íšŒ (íì¶œí˜ˆ, ê´€ì ˆì—¼ ë“± ì£¼ìš” ì§ˆí™˜ í•„í„°ë§)
        """
        if not self.api_key or len(self.api_key) < 10:
            return []

        params = {
            "hrNo": hr_no,
            "html": False  # JSON ìš”ì²­ ê°€ì •ì„ ìœ„í•¨ (ì‹¤ì œë¡œëŠ” XMLì¼ ìˆ˜ ìˆìŒ, ê³µê³µë°ì´í„° í¬ë§· í™•ì¸ í•„ìš”)
        }
        # ê³µê³µë°ì´í„°í¬í„¸ API18_1 í˜¸ì¶œ
        items = self._call_api(config.MEDICAL_API, params, tag="ì§„ë£Œì •ë³´")
        
        history = []
        if items:
            # ì£¼ìš” ì§ˆí™˜ í‚¤ì›Œë“œ
            keywords = ["ì¶œí˜ˆ", "í", "ê´€ì ˆ", "ì¸ëŒ€", "ê³¨ì ˆ", "íŒŒí–‰", "ê±´ì—¼"]
            
            for item in items:
                ill_name = item.get("illName", "")
                treat_date = item.get("treaDt", "")
                
                if any(k in ill_name for k in keywords):
                    history.append(f"{treat_date}: {ill_name}")
        
        return history[:5] # ìµœê·¼ 5ê±´ë§Œ ë°˜í™˜

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 4. ê²½ì£¼ ê²°ê³¼ (ë³µê¸°/ì‹¬íŒ ë¦¬í¬íŠ¸) - API + Web Scraping
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def fetch_race_results(self, race_date: str, meet: str = "1",
                           race_no: str = None) -> pd.DataFrame:
        """
        ê²½ì£¼ ê²°ê³¼ ë°ì´í„° ìˆ˜ì§‘ (API -> Web Fallback)
        """
        print(f"[Info] ê²½ì£¼ ê²°ê³¼ ìˆ˜ì§‘ ì¤‘... (ë‚ ì§œ: {race_date})")

        if self.api_key and len(self.api_key) > 10:
            params = {"rc_date": race_date, "meet": meet}
            if race_no:
                params["rcNo"] = race_no
            items = self._call_api(config.RACE_RESULT_API, params, tag="ê²½ì£¼ê²°ê³¼")
            if items:
                df = pd.DataFrame(items)
                
                # ë‚ ì§œ ê²€ì¦ (APIê°€ ë‚ ì§œ íŒŒë¼ë¯¸í„°ë¥¼ ë¬´ì‹œí•˜ê³  ìµœì‹  ë°ì´í„°ë¥¼ ì£¼ëŠ” ê²½ìš°ê°€ ìˆìŒ)
                # raceDt ì»¬ëŸ¼ í™•ì¸
                date_col = next((c for c in df.columns if c.lower() in ["racedt", "rcdate", "rc_date"]), None)
                if date_col:
                    # ì²« ë²ˆì§¸ í–‰ì˜ ë‚ ì§œ í™•ì¸ (ë¬¸ìì—´ ë³€í™˜ í›„ ë¹„êµ)
                    api_date = str(df.iloc[0][date_col]).replace("-", "").replace(".", "")
                    req_date = str(race_date).replace("-", "").replace(".", "")
                    
                    if api_date != req_date:
                        print(f"  [Warning] API ë°˜í™˜ ë‚ ì§œ({api_date})ê°€ ìš”ì²­ ë‚ ì§œ({req_date})ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                        print("  [Info] API ë°ì´í„° ë‚ ì§œ ë¶ˆì¼ì¹˜. ì›¹ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                        # [FIX] Do NOT return 'df' here. Fall through to scraping.
                    else:
                        if not df.empty:
                            print(f"  [Success] ê²½ì£¼ ê²°ê³¼ {len(df)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ (API)")
                            return df
                            
            # If we are here, it means API failed or Date Mismatch occurred.
            # Fallback to scraping happens below because we didn't return.

        # API í˜¸ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° ì—†ìŒ
        print("  [Info] ê²½ì£¼ ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ê²½ì£¼ ì „ì´ê±°ë‚˜ ë°ì´í„° ë¯¸ì œê³µ)")
        
        # [Fallback] Web Scraping for Race Results
        print("  [Info] ì›¹ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ê²½ì£¼ ê²°ê³¼ ìˆ˜ì§‘ ì‹œë„...")
        return self._scrape_results_full(race_date, meet)

    
    def _parse_dividend(self, dfs) -> dict:
        """
        ê²°ê³¼ HTMLì—ì„œ ë°°ë‹¹ë¥  ì •ë³´ ì¶”ì¶œ (ë³µìŠ¹, ì‚¼ë³µìŠ¹ ë“±)
        """
        dividends = {"qui": 0.0, "trio": 0.0}
        
        # ì¼ë°˜ì ìœ¼ë¡œ 3ì—´ ì´ìƒì´ê³  "ë³µìŠ¹" ë‹¨ì–´ê°€ í¬í•¨ëœ í…Œì´ë¸” ê²€ìƒ‰
        for df in dfs:
            try:
                # í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰
                text_content = df.to_string()
                
                # ë³µìŠ¹ ë°°ë‹¹ íŒŒì‹±
                # í…Œì´ë¸” êµ¬ì¡°: Row 1 (Index 1) -> Col 1 (ë³µìŠ¹ ë°°ë‹¹)
                # ë‹¨, ì •í™•í•œ ìœ„ì¹˜ëŠ” ê°€ë³€ì ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í‚¤ì›Œë“œ ê²€ìƒ‰ ë˜ëŠ” ê³ ì • ìœ„ì¹˜ ì‹œë„
                
                # [Strategy] ê³ ì • ìœ„ì¹˜ ê°€ì • (Table 6 in debug logs)
                # Shape (4, 3) í™•ì¸
                if df.shape == (4, 3):
                    val_qui = str(df.iloc[1, 1])
                    val_trio = str(df.iloc[2, 2])
                    
                    # [Check] ë§¤ì¶œì•¡ í…Œì´ë¸”(ì½¤ë§ˆ í¬í•¨) ì œì™¸
                    if "," in val_qui or "," in val_trio:
                        continue
                    
                    # ìˆ«ìë§Œ ì¶”ì¶œ (ì •ê·œì‹)
                    import re
                    
                    # Quinella
                    match_q = re.search(r"(\d+(\.\d+)?)", val_qui)
                    if match_q:
                        dividends["qui"] = float(match_q.group(1))
                        
                    # Trio
                    match_t = re.search(r"(\d+(\.\d+)?)", val_trio)
                    if match_t:
                        dividends["trio"] = float(match_t.group(1))
                        
                    # ì°¾ì•˜ìœ¼ë©´ ì¤‘ë‹¨ (ê°€ì¥ ìœ ë ¥í•œ í…Œì´ë¸” í•˜ë‚˜ë§Œ ë´„)
                    if dividends["qui"] > 0 or dividends["trio"] > 0:
                        break
            except Exception as e:
                continue
                
        return dividends

    def _scrape_results_full(self, race_date: str, meet: str) -> pd.DataFrame:
        """KRA ì›¹ì‚¬ì´íŠ¸ ê²½ì£¼ì„±ì í‘œ ìŠ¤í¬ë˜í•‘ (ìƒì„¸ì •ë³´ í¬í•¨ì„ ìœ„í•´ ë°˜ë³µ ìš”ì²­)"""
        try:
            # 1. ë¨¼ì € ì „ì²´ ê²½ì£¼ ëª©ë¡(ê°¯ìˆ˜) í™•ì¸ì„ ìœ„í•´ List í˜ì´ì§€ ì¡°íšŒ
            list_url = "https://race.kra.co.kr/raceScore/ScoretableScoreList.do"
            params = {"meet": meet, "realRcDate": race_date}
            
            # [Fix] Referer í—¤ë” ì¶”ê°€ (í•„ìš” ì‹œ)
            headers = {
                "Referer": "https://race.kra.co.kr/raceScore/ScoretableScoreList.do"
            }
            
            resp = self.session.get(list_url, params=params, headers=headers, timeout=10)
            resp.raise_for_status()
            
            # ê²½ì£¼ ë²ˆí˜¸(1, 2, 3...) ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ìµœëŒ€ ê²½ì£¼ ìˆ˜ íŒŒì•…
            # ì˜ˆ: onclick="ScoreDetailPopup('1','20240302','1');"
            import re
            pattern = r"ScoreDetailPopup\s*\(\s*['\"]" + str(meet) + r"['\"]\s*,\s*['\"]" + str(race_date) + r"['\"]\s*,\s*['\"](\d+)['\"]\s*\)"
            matches = re.findall(pattern, resp.text)
            
            if not matches:
                print("  [Info] ê²½ì£¼ ê°¯ìˆ˜ íŒŒì•… ì‹¤íŒ¨ (íŒ¨í„´ ë§¤ì¹­ ì—†ìŒ). 1~12ê²½ì£¼ ìˆœì°¨ ì‹œë„.")
                race_nos = [str(i) for i in range(1, 13)]
            else:
                # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
                race_nos = sorted(list(set(matches)), key=lambda x: int(x))
                print(f"  [Info] ì´ {len(race_nos)}ê°œ ê²½ì£¼ ê°ì§€ ({race_nos})")

            all_results = []
            
            # 2. ê° ê²½ì£¼ë³„ ìƒì„¸ ì„±ì  ì¡°íšŒ
            detail_url = "https://race.kra.co.kr/raceScore/ScoretableDetailList.do"
            
            for rc_no in race_nos:
                print(f"    - Scraping Race {rc_no}...")
                post_data = {
                    "meet": meet,
                    "realRcDate": race_date,
                    "realRcNo": rc_no
                }
                
                try:
                    resp_detail = self.session.post(detail_url, data=post_data, headers=headers, timeout=10)
                    resp_detail.raise_for_status()
                    resp_detail.encoding = 'euc-kr' # [Fix] Encoding
                    
                    dfs = pd.read_html(StringIO(resp_detail.text), flavor='lxml')
                    
                    # [Added] ë°°ë‹¹ë¥  íŒŒì‹±
                    dividends = self._parse_dividend(dfs)
                    
                    target_df = None
                    
                    # ì›í•˜ëŠ” í…Œì´ë¸” ì°¾ê¸°: "ìˆœìœ„", "ë§ˆëª…" í¬í•¨
                    for i, df in enumerate(dfs):
                        cols = [str(c) for c in df.columns]
                        if any("ìˆœìœ„" in c for c in cols) and any("ë§ˆëª…" in c for c in cols):
                            target_df = df
                            break
                    
                    if target_df is not None:
                        # [Added] ê³ ìœ  ë§ˆë²ˆ(hrId) ì¶”ì¶œ (BeautifulSoup í•„ìš”)
                        try:
                            soup = BeautifulSoup(resp_detail.text, "html.parser")
                            # ë§ˆëª…ì´ í¬í•¨ëœ í…Œì´ë¸” ì°¾ê¸°
                            tables = soup.find_all("table")
                            hr_id_map = {}
                            
                            for tbl in tables:
                                if "ë§ˆëª…" in tbl.get_text():
                                    links = tbl.find_all("a")
                                    for lnk in links:
                                        # onclick="FnPopHorseDetail('0033667', ...)"
                                        onclick = lnk.get("onclick", "")
                                        if "PopHorseDetail" in onclick:
                                            # ë§ˆëª… ì¶”ì¶œ (ê³µë°± ì œê±°)
                                            name = lnk.get_text(strip=True)
                                            # ID ì¶”ì¶œ
                                            match = re.search(r"PopHorseDetail\s*\(\s*['\"](\d+)['\"]", onclick)
                                            if match:
                                                hr_id_map[name] = match.group(1)
                            
                            if hr_id_map:
                                # target_dfì˜ ë§ˆëª… ì»¬ëŸ¼ ì •ë¦¬
                                target_df["_clean_name"] = target_df["ë§ˆëª…"].astype(str).str.strip().str.replace(r"\s+", "", regex=True)
                                target_df["hrId"] = target_df["_clean_name"].map(lambda x: hr_id_map.get(x, ""))
                                target_df.drop(columns=["_clean_name"], inplace=True)
                                # print(f"      [Debug] Extracted {len(hr_id_map)} Unique IDs")
                        except Exception as e:
                            print(f"      [Warn] Unique ID extraction failed: {e}")

                        # ê²½ì£¼ ë²ˆí˜¸ ì»¬ëŸ¼ ì¶”ê°€
                        target_df["rcNo"] = rc_no
                        
                        # ì»¬ëŸ¼ ë§¤í•‘ (ê³µë°±/ì¤„ë°”ê¿ˆ ì œê±° í›„)
                        target_df.columns = [str(c).replace("\n", "").replace(" ", "") for c in target_df.columns]
                        # [Added] ë°°ë‹¹ë¥  ì •ë³´ ì¶”ê°€
                        target_df["qui_div"] = dividends.get("qui", 0.0)
                        target_df["trio_div"] = dividends.get("trio", 0.0)

                        # ì „ì²˜ë¦¬
                        target_df = target_df.rename(columns={
                            "ìˆœìœ„": "ord", 
                            "ì°©ìˆœ": "ord",
                            "ë§ˆë²ˆ": "hrNo", 
                            "ë§ˆëª…": "hrName", 
                            "ì‚°ì§€": "prodName",
                            "ì„±ë³„": "sex",
                            "ì—°ë ¹": "age",
                            "ì¤‘ëŸ‰": "wgBudam", "ë¶€ë‹´ì¤‘ëŸ‰": "wgBudam",
                            "ê¸°ìˆ˜ëª…": "jkName", "ê¸°ìˆ˜": "jkName",
                            "ì¡°êµì‚¬ëª…": "trName", "ì¡°êµì‚¬": "trName",
                            "ë§ˆì£¼ëª…": "owName", "ë§ˆì£¼": "owName",
                            "ê¸°ë¡": "rcTime", "ì£¼í–‰ê¸°ë¡": "rcTime", "ê²½ì£¼ê¸°ë¡": "rcTime",
                            "ì°©ì°¨": "diff", 
                            "ë§ˆì²´ì¤‘": "wgHr", "ì²´ì¤‘": "wgHr",
                            "ë‹¨ìŠ¹": "winOdds",
                            "ì—°ìŠ¹": "plcOdds",
                            "S1F": "s1f", "G1F": "g1f", "G-1F": "g1f", "3C": "g3f", "4C": "g1f" # ê·¼ì‚¬ ë§¤í•‘
                        })
                        
                        # ìˆœìœ„ ë°ì´í„° ì •ì œ (ì·¨ì†Œ, ì¤‘ì§€ ë“± ì²˜ë¦¬)
                        if "ord" in target_df.columns:
                            target_df["ord"] = pd.to_numeric(target_df["ord"], errors="coerce").fillna(99).astype(int)
                            
                        # ë§ˆë²ˆ ì •ì œ
                        if "hrNo" in target_df.columns:
                            target_df["hrNo"] = pd.to_numeric(target_df["hrNo"], errors="coerce").fillna(0).astype(int).astype(str)

                        all_results.append(target_df)
                    else:
                        print(f"      [Warn] Race {rc_no}: No result table found.")
                        
                    time.sleep(0.3) # ë”œë ˆì´
                    
                except Exception as e:
                    print(f"      [Error] Race {rc_no} scraping failed: {e}")

            if all_results:
                final_df = pd.concat(all_results, ignore_index=True)
                print(f"  [Success] ì´ {len(final_df)}ê±´ì˜ ê²½ì£¼ ì„±ì  ìˆ˜ì§‘ ì™„ë£Œ")
                return final_df
            
            return pd.DataFrame()

        except Exception as e:
            print(f"  [Error] ê²½ì£¼ ê²°ê³¼ ìŠ¤í¬ë˜í•‘ ì „ì²´ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 5. ë§ˆì²´ì¤‘ ì •ë³´
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def fetch_horse_weight(self, race_date: str, meet: str = "1") -> pd.DataFrame:
        """
        ë‹¹ì¼ ë§ˆì²´ì¤‘ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        (ë§ˆì²´ì¤‘ì€ ê²½ì£¼ ë‹¹ì¼ ê³µê°œë˜ë¯€ë¡œ ì¶œì „í‘œì— í¬í•¨ë˜ëŠ” ê²½ìš°ë„ ìˆìŒ)
        """
        print(f"âš– ë§ˆì²´ì¤‘ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")

        # ì¶œì „í‘œ APIì— ë§ˆì²´ì¤‘ì´ í¬í•¨ëœ ê²½ìš° í™œìš©
        entries = self.fetch_race_entries(race_date, meet)
        if not entries.empty and "wgHr" in entries.columns:
            weight_df = entries[["hrName", "hrNo", "rcNo", "wgHr"]].copy()
            weight_df.rename(columns={"wgHr": "weight"}, inplace=True)
            print(f"  [Success] ë§ˆì²´ì¤‘ {len(weight_df)}ê±´ í™•ì¸")
            return weight_df

        return pd.DataFrame()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # í†µí•© ë°ì´í„° ìˆ˜ì§‘
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def collect_all(self, race_date: str, meet: str = "1") -> dict:
        """
        ê²½ì£¼ì¼ ê¸°ì¤€ ëª¨ë“  ë°ì´í„°ë¥¼ í†µí•© ìˆ˜ì§‘í•©ë‹ˆë‹¤.

        Returns:
            dict with keys: entries, training, results, weights
        """
        print(f"\n{'='*60}")
        print(f"ğŸ KRA ë°ì´í„° í†µí•© ìˆ˜ì§‘ ì‹œì‘")
        print(f"   ë‚ ì§œ: {race_date} | ê²½ë§ˆì¥: {config.MEET_CODES.get(meet, meet)}")
        print(f"{'='*60}\n")

        data = {}

        # 1) ì¶œì „í‘œ
        entries_df = self.fetch_race_entries(race_date, meet)
        
        # [Improvement] ì¶œì „í‘œì— ê³¼ê±° ê¸°ë¡(s1f_1, ord_1 ë“±)ì´ ì—†ìœ¼ë©´,
        # ê°œë³„ ë§ˆí•„ ìƒì„¸ì •ë³´ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ì—¬ ë³‘í•© (ì‹œê°„ ì†Œìš”ë¨)
        if not entries_df.empty:
            print(f"  [Debug] Entries columns: {entries_df.columns.tolist()[:5]}...")
            if "s1f_1" not in entries_df.columns:
                print("  [Info] ì¶œì „í‘œì— ê³¼ê±° ê¸°ë¡ ë¶€ì¬ -> ë§ˆí•„ë³„ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ë° ë³‘í•© ì‹œë„ (ì‹œê°„ ì†Œìš” ì˜ˆìƒ)")
                entries_df = self._enrich_entries_with_history(entries_df, race_date, meet)
            else:
                print("  [Info] ì¶œì „í‘œì— ê³¼ê±° ê¸°ë¡ ì¡´ì¬ (Enrichment Skip)")
        
        data["entries"] = entries_df

        # 2) ì¡°êµ ë°ì´í„° (ìµœê·¼ 1ì£¼)
        data["training"] = self.fetch_training_for_week(race_date, meet)

        # 3) [Modified] ì§ì „ ê²½ì£¼ ê²°ê³¼ (Track Bias ìš©ë„ì˜€ìœ¼ë‚˜ í˜¼ë€ ì•¼ê¸° -> ì œê±°)
        # ë§ë“¤ì˜ ê³¼ê±° ì„±ì ì€ ê°œë³„ ë§ˆí•„ ìƒì„¸ì •ë³´(fetch_horse_details)ì—ì„œ í™•ë³´í•¨.
        # 3. ê²½ì£¼ ê²°ê³¼ (Backtesting ìš©ë„)
        data["results"] = self.fetch_race_results(race_date, meet)

        # 4) ë§ˆì²´ì¤‘
        data["weights"] = self.fetch_horse_weight(race_date, meet)

        # ìºì‹œ ì €ì¥
        self._save_cache(race_date, meet, data)

        print(f"\n{'='*60}")
        print(f"[Success] ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        collected = {k: len(v) for k, v in data.items() if isinstance(v, pd.DataFrame) and not v.empty}
        for k, v in collected.items():
            print(f"   {k}: {v}ê±´")
        print(f"{'='*60}\n")

        return data

    def _enrich_entries_with_history(self, entries_df: pd.DataFrame, race_date: str, meet: str) -> pd.DataFrame:
        """ì¶œì „í‘œì˜ ê° ë§ˆí•„ì— ëŒ€í•´ ê³¼ê±° 3~5ì „ ê¸°ë¡ì„ ì¡°íšŒí•˜ì—¬ s1f_1, ord_1 ë“±ì˜ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€"""
        print(f"  [Enrich] ê³¼ê±° ì„±ì  ë°ì´í„° ë³‘í•© ì‹œì‘ (ì´ {len(entries_df)}ë§ˆë¦¬)")
        
        enriched_rows = []
        if "hrNo" not in entries_df.columns:
            return entries_df

        # ì´ë¯¸ ì²˜ë¦¬í•œ ë§ˆë²ˆ ìºì‹± (ì¤‘ë³µ ë°©ì§€)
        history_cache = {}
        # [Added-DarkHorse] ì‹¬íŒ ë¦¬í¬íŠ¸ ìºì‹± (date, rcNo) -> {hrName: report}
        steward_db = {} 

        count = 0
        total = len(entries_df)

        for idx, row in entries_df.iterrows():
            # [Improvement] Unique ID(hrId)ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ë§ˆë²ˆ(hrNo) ì‚¬ìš©
            # hrNoëŠ” ê²Œì´íŠ¸ ë²ˆí˜¸ë¼ ë¶€ì •í™•í•˜ì§€ë§Œ, hrIdê°€ ì—†ëŠ” ê²½ìš° ì–´ì©” ìˆ˜ ì—†ìŒ
            hr_id = str(row.get("hrId", ""))
            gate_no = str(row.get("hrNo", ""))
            
            target_id = hr_id if hr_id and hr_id != "nan" else gate_no

            # ì‹ë³„ìê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if not target_id or target_id == "nan" or target_id == "0":
                enriched_rows.append(row)
                continue
                
            if target_id in history_cache:
                hist_df = history_cache[target_id]
            else:
                try:
                    # ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘ (list[dict] ë°˜í™˜)
                    records = self._scrape_horse_details(target_id, meet)
                    hist_df = pd.DataFrame(records) if records else pd.DataFrame()
                except Exception as e:
                    hist_df = pd.DataFrame()
                
                history_cache[target_id] = hist_df
                time.sleep(0.1)

            # Merge into row
            new_row = row.copy()
            
            if not hist_df.empty:
                if "rcDate" in hist_df.columns:
                    hist_df["rcDate"] = hist_df["rcDate"].astype(str).str.replace("-", "").str.replace(".", "")
                    hist_df = hist_df.sort_values("rcDate", ascending=False)
                
                current_date = str(race_date).replace("-", "")
                
                valid_hist = []
                steward_db = {} # (date, rcNo) -> {hrName: report_text}

                for _, h_row in hist_df.iterrows():
                    h_date = str(h_row.get("rcDate", ""))
                    if h_date < current_date:
                        valid_hist.append(h_row)
                        if len(valid_hist) >= 5: # ìµœê·¼ 5ì „
                            break
                            
                # [Added-DarkHorse] ê°€ì¥ ìµœê·¼ ê²½ì£¼ì˜ ì‹¬íŒ ë¦¬í¬íŠ¸ ì¡°íšŒ (ë¶ˆìš´ë§ˆ íƒì§€ìš©)
                if valid_hist:
                    last_race = valid_hist[0]
                    l_date = str(last_race.get("rcDate", ""))
                    l_no = str(last_race.get("rcNo", ""))
                    
                    if l_date and l_no:
                        # ìºì‹œ í™•ì¸
                        cache_key = (l_date, l_no)
                        if cache_key not in steward_db:
                            # í•´ë‹¹ ê²½ì£¼ì˜ ë¦¬í¬íŠ¸ ì „ì²´ ìˆ˜ì§‘
                            try:
                                # meetëŠ” ë™ì¼í•˜ë‹¤ê³  ê°€ì • (ì„œìš¸->ì„œìš¸). êµì°¨ê²½ì£¼ëŠ” ë³µì¡í•˜ë¯€ë¡œ ì¼ë‹¨ íŒ¨ìŠ¤
                                reports_map = self.scrape_steward_reports(l_date, meet, l_no)
                                # hrName ê¸°ì¤€ìœ¼ë¡œ ì¬ë§¤í•‘
                                name_map = {}
                                for _, r_list in reports_map.items():
                                    for r in r_list:
                                        # r = {'date':..., 'report':..., 'hrName':...}
                                        name_map[r['hrName']] = r['report']
                                steward_db[cache_key] = name_map
                            except:
                                steward_db[cache_key] = {}
                        
                        # ë‚´ ì´ë¦„ìœ¼ë¡œ ë¦¬í¬íŠ¸ ì°¾ê¸°
                        my_name = str(row.get("hrName", "")).strip()
                        if my_name in steward_db[cache_key]:
                            new_row["steward_report_1"] = steward_db[cache_key][my_name]
                        else:
                            new_row["steward_report_1"] = ""

                            
                for i, h_row in enumerate(valid_hist, 1):
                    cols_to_fetch = ["s1f", "g1f", "ord", "rcTime", "wgBudam", "rating", "rcNo", "rcDate", "weight"]
                    for col in cols_to_fetch:
                        val = h_row.get(col, "")
                        new_row[f"{col}_{i}"] = val
            
            enriched_rows.append(new_row)
            count += 1
            if count % 10 == 0:
                print(f"    - {count}/{total} ì²˜ë¦¬ ì¤‘...", end="\r")

        print(f"  [Enrich] ì™„ë£Œ.                               ")
        return pd.DataFrame(enriched_rows)

    def _save_cache(self, race_date: str, meet: str, data: dict):
        """ìˆ˜ì§‘ ë°ì´í„°ë¥¼ CSV ìºì‹œë¡œ ì €ì¥"""
        cache_dir = os.path.join(config.DATA_DIR, f"{race_date}_{meet}")
        os.makedirs(cache_dir, exist_ok=True)

        for key, df in data.items():
            if isinstance(df, pd.DataFrame) and not df.empty:
                path = os.path.join(cache_dir, f"{key}.csv")
                df.to_csv(path, index=False, encoding="utf-8-sig")
                print(f"  ğŸ’¾ ìºì‹œ ì €ì¥: {path}")

    def load_cache(self, race_date: str, meet: str) -> dict:
        """ìºì‹œëœ ë°ì´í„° ë¡œë“œ"""
        cache_dir = os.path.join(config.DATA_DIR, f"{race_date}_{meet}")
        data = {}

        if not os.path.exists(cache_dir):
            return data

        for name in ["entries", "training", "results", "weights"]:
            path = os.path.join(cache_dir, f"{name}.csv")
            if os.path.exists(path):
                data[name] = pd.read_csv(path, encoding="utf-8-sig")
                print(f"  ğŸ“‚ ìºì‹œ ë¡œë“œ: {name} ({len(data[name])}ê±´)")

        return data


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‹¨ë… ì‹¤í–‰ í…ŒìŠ¤íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    scraper = KRAScraper()

    # í…ŒìŠ¤íŠ¸: ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€
    today = datetime.now().strftime("%Y%m%d")
    print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰ â€” ë‚ ì§œ: {today}\n")

    # ì¶œì „í‘œ í…ŒìŠ¤íŠ¸
    entries = scraper.fetch_race_entries(today, "1")
    if not entries.empty:
        print(entries.head())
    else:
        print("ì¶œì „í‘œ ë°ì´í„° ì—†ìŒ (ê²½ì£¼ì¼ì´ ì•„ë‹ ìˆ˜ ìˆìŒ)")

    # ì¡°êµ ë°ì´í„° í…ŒìŠ¤íŠ¸
    training = scraper.fetch_training_data(today, "1")
    if not training.empty:
        print(training.head())
